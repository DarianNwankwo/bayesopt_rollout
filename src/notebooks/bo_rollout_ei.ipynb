{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Myopic Bayesian Optimization: Towards Non-Trivial Horizons \n",
    "For the sake of simplicity, we'll focus first on 1D problems and then generalize of programming framework to hand nD problems.\n",
    "\n",
    "## Questions & Concerns\n",
    "- If we return functional objects, we can simulate having a handle on our Gaussian Process as it evolves in time cheaply by evaluating each instance\n",
    "  on an as needed basis. The most costly thing would be having to keep track of the handles to such in some type of dictionary.\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "- [x] Finish LaTeX on analytic formulation of expected improvement in 1d.\n",
    "- [x] Write code to implement expected improvement in 1d.\n",
    "- [ ] Once the BO loop can solve this problem in 1d, work on computing trajectories and add code to support such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Optim\n",
    "using Plots\n",
    "using Statistics\n",
    "using Zygote\n",
    "\n",
    "import Base.@kwdef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Kernel end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessory and Utility Methods + Black Box Process\n",
    "A pretty laborious thing to code is the construction of our covariances matrices with respect to different variates, so we abstract away this process. Provided that\n",
    "you specify your kernel of choice and provide it access to a collection of both your variates, then we can easily construct such a matrix. We also accept a noise\n",
    "parameter, however, I believe this should be encapsulated elsewhere (for storage purposes) because I'm sure we'll need access to it again.\n",
    "\n",
    "Here, we assume that our `Black Box Process` is known, although, in practice, it isn't. We assume a strict analytical form, of which, we can sample from with or\n",
    "without noise over the interval $[-1, 2]$.\n",
    "$$\n",
    "f_\\sigma (x) = \\alpha \\cdot (-sin(3x) -x^2 + .7x) + \\sigma \\epsilon\n",
    "$$\n",
    "where $\\alpha \\in \\mathbb{R}$ is a scale factor and $\\epsilon \\sim \\mathcal{N}(0, 1)$\n",
    "\n",
    "\n",
    "#### CovarianceMatrix\n",
    "Arguments:\n",
    "* kernel::Kernel: a kernel functor that evaluates the covariance between two vectors\n",
    "* Xa::Array{Float64, 2}: a matrix of variates\n",
    "* Xb::Array{Float64, 2}: a matrix of variates\n",
    "* $\\sigma$noise::Real: noise term \n",
    "\n",
    "Constructs a covariance matrix between the given matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toMatrix (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function blackBoxProcess(xloc::Float64; σnoise::Float64=.1)\n",
    "    # Domain of interest (for f) is restricted to [-1, 2]\n",
    "    scale_factor = 1.\n",
    "    horizontal_shift = 0\n",
    "    f(x; σ=σnoise) = scale_factor * (-sin(3x) - x^2 + .7x + horizontal_shift) + σ*randn()\n",
    "    return f(xloc)\n",
    "end\n",
    "\n",
    "# Matrix will be xlength x ylength: every row corresponds to an element from x\n",
    "# More generally, the covariance between two entities can be vector valued\n",
    "function covarianceMatrix(kernel::Kernel, Xa::Array{Float64, 2}, Xb::Array{Float64, 2}; σnoise::Real=0.0)\n",
    "    @assert size(Xa)[1] == size(Xb)[1]\n",
    "    @assert isempty(methods(kernel)) == false\n",
    "    \n",
    "    # Kernel should be a function of two arguments\n",
    "    xalength = size(Xa)[2] # rows here correspond to dimensionality\n",
    "    xblength = size(Xb)[2]\n",
    "    covMatrix = zeros(xalength, xblength)\n",
    "\n",
    "    for xacol in 1:xalength\n",
    "        for xbcol in 1:xblength\n",
    "            δkronecker = xacol == xbcol ? σnoise^2 * 1 : 0\n",
    "            covMatrix[xacol, xbcol] = kernel(Xa[:, xacol], Xb[:, xbcol]) + δkronecker\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return covMatrix\n",
    "end\n",
    "\n",
    "function toMatrix(x::Float64)\n",
    "    return reshape([x], (1, 1))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels as Structs\n",
    "Here, we represent support for each kernel as a struct. Each struct accepts hyperparameters, associated with the respective kernel, as named arguments\n",
    "and returns a handle on the object. Each object is callable and represents computing $k(x_a, x_b)$. Every kernel has an output variance/scale factor associated with, which we'll call `variance`. We also take into account noisy situations as well which is denoted as `noise` and is squared\n",
    "internally.\n",
    "\n",
    "`Squared Exponential Kernel`: $k(x, x') = \\nu^2 exp(-\\frac{(x - x')^2}{2l^2}) + \\sigma_{noise}^2 \\delta_{xx'}$\n",
    "\n",
    "### Design Choices\n",
    "I want each kernel's hyperparameters to be name specified to avoid ambuguity with ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SquaredExponential"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@kwdef mutable struct SquaredExponential <: Kernel\n",
    "    variance::Real # provided as ν²\n",
    "    lengthscale::Real # provided as l\n",
    "    noise::Real # provided as σ\n",
    "    \n",
    "    function SquaredExponential(variance, lengthscale, noise)\n",
    "        @assert variance >= 0 \"variance must be a positive real number\"\n",
    "        \n",
    "        return new(variance, lengthscale, noise)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SquaredExponential() = SquaredExponential(variance=1.0, lengthscale=1.0, noise=0.0)\n",
    "SE() = SquaredExponential()\n",
    "\n",
    "function (se::SquaredExponential)(x::Array{Float64, 1}, y::Array{Float64, 1})::Real\n",
    "    δxx = Int8(x == y)\n",
    "    r = x - y\n",
    "    ρ = norm(r, 2)\n",
    "    return se.variance * exp(-ρ^2 / 2*se.lengthscale^2) + δxx*se.noise^2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×201 Array{Float64,2}:\n",
       " -107.988  -105.929  -103.803  -101.615  …  -88.2774  -90.0905  -92.012"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppose we have some process and we have n known function evaluations\n",
    "n = 2\n",
    "d = 1\n",
    "σnoise = 0.05\n",
    "\n",
    "# D x n, D: dimensions, n: number of observations\n",
    "X = round.(\n",
    "    rand(Uniform(-1., 2.), d, n), \n",
    "    digits=4\n",
    ") \n",
    "y = vec(blackBoxProcess.(X; σnoise=σnoise))\n",
    "\n",
    "# Let's sample from our predictive distribution conditioned on our observations\n",
    "# across the interval [-1, 2] to observe some reasonable functions\n",
    "Xtrue = collect(-10.0:.1:10.0)\n",
    "Xtrue = reshape(Xtrue, (1, length(Xtrue)))\n",
    "ytrue = blackBoxProcess.(Xtrue; σnoise=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes as Structs\n",
    "It'd be very convenient for us to encapsulate information associated with GPs into a struct. It's immediately obvious, that the predictive mean and \n",
    "predictive variance rely on similar information and computing and storing such information multiple times is redundant and can become expensive in\n",
    "higher dimensional settings. I'm making a design choice here of accepting the kernel and mean function as function objects instead of vectors. Multiple constructor methods will be supported, but for the time-being, we'll focus on one.\n",
    "\n",
    "*Gaussian Process State*\n",
    "\n",
    "`Mean Function`: a function like object representing the mean of the observations\n",
    "\n",
    "`Covariance Function`: a function like object used to compute the covariance matrix between observations\n",
    "\n",
    "`Cholesky Matrix`: from the cholesky factor, we should be able to reconstruct our covariance matrix since the cholesky factorization of a positive semi-definite matrix is \n",
    "unique.\n",
    "\n",
    "`Cholesky Solve (alpha)`: $\\alpha := L^T \\backslash (L \\backslash y)$,\n",
    "\n",
    "### Gaussian Process Utility Methods\n",
    "`Conditional`: conditioning the GP on inputs and targets should take the internal covariance function and compute the predictive mean and predictive variance\n",
    "* Or, we compute the alpha term which is then used by our predictive variance and predictive mean. So, each time we return a handle on the predictive mean and predictive\n",
    "  variance, we ensure that the alpha term has been computed (which will take place in conditional). We may\n",
    "\n",
    "`Predictive Mean`: a function that accepts a GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Union{Nothing, Type} where Type"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper method for ensuring an object is callable\n",
    "isCallable(f) = !isempty(methods(f))\n",
    "const DataOrNothing{Type} = Union{Type, Nothing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianProcess"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@kwdef mutable struct GaussianProcess\n",
    "    meanFunction\n",
    "    covarianceFunction::Kernel\n",
    "    choleskyFactor::DataOrNothing{AbstractMatrix{Float64}}\n",
    "    choleskySolve::DataOrNothing{AbstractVector{Float64}}\n",
    "    predictiveVariance\n",
    "    predictiveMean\n",
    "    \n",
    "    function GaussianProcess(mean, kernel)\n",
    "        @assert isCallable(mean) == true\n",
    "        @assert isCallable(kernel) == true\n",
    "\n",
    "        return new(\n",
    "            mean,\n",
    "            kernel,\n",
    "            nothing,\n",
    "            nothing,\n",
    "            nothing,\n",
    "            nothing\n",
    "        )\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GP (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GP(mean, kernel) = GaussianProcess(mean, kernel)\n",
    "# add a constructor method for accepting the other factors and returns a new copy\n",
    "# of a GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "covarianceMatrix (generic function with 2 methods)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function covarianceMatrix(gp::GaussianProcess)::Union{AbstractMatrix{Float64}, Nothing}\n",
    "    if isnothing(gp.choleskyFactor) return nothing end\n",
    "    return gp.choleskyFactor * gp.choleskyFactor'\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization as Structs\n",
    "In order to perform the iteration, efficient utilization of state is necessary. Abstracting away unnecessary state from our Gaussian Process is necessary. Mathematically,\n",
    "a Gaussian Process is completely defined by it's mean function and covariance function. Thus, programmatically, we'll leave it the same. It does need data to perform useful\n",
    "operations, so we'll provide that by wrapping the iteration into a Bayesian Optimization struct and maintaining supplementary state here.\n",
    "\n",
    "*Bayesian Optimization State*\n",
    "\n",
    "`Samples/Variates X`: We enforce that the observations provided are $\\mathbb{R}^{D \\times n}$, where $D := $ the dimensionality of the variates and $n := $ is the number of\n",
    "observations.\n",
    "\n",
    "`Values/Covariates y`: We further enforce that the observations are provided as column vector $v \\in \\mathbb{R}^{n}$\n",
    "\n",
    "`Best Observation ybest`: Since we're usually dealing with sparse data initially, we find the best value from the given covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianOptimization"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@kwdef mutable struct BayesianOptimization\n",
    "    gp::GaussianProcess\n",
    "    X::DataOrNothing{AbstractMatrix{Float64}}\n",
    "    y::DataOrNothing{AbstractVector{Float64}}\n",
    "    ybest::Float64\n",
    "    xi::Float64 # exploration parameter\n",
    "    \n",
    "    function BayesianOptimization(gp::GaussianProcess, X::Array{Float64, 2}, y::Array{Float64, 1})\n",
    "        @assert size(X)[2] == length(y) \"each variate must have a corresponding covariate:\" # may cause issues when generalizing to vector valued functions\n",
    "        \n",
    "        return new(gp, X, y, maximum(y)) \n",
    "    end\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BO (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BO(gp, X, y) = BayesianOptimization(gp, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conditional (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sample(gp::GaussianProcess, xloc::Float64)\n",
    "    # Want to sample from a multivariate normal which is centered at the predictive\n",
    "    # mean with covariance matrix defined by the GPs kernel matrix\n",
    "    # To generate a sample at location xloc, we must compute the predictive mean and predictive variance\n",
    "    # at the location and sample from a mvn with mean and covariance specified as such\n",
    "    μxloc = gp.predictiveMean(xloc)\n",
    "    σxloc = gp.predictiveVariance(xloc)\n",
    "    uninorm = Normal(μxloc, σxloc^2)\n",
    "    return nsamples::Int64 -> rand(uninorm, nsamples)\n",
    "end\n",
    "\n",
    "# function pseudoConditional(gp::GaussianProcess, x::Array{Float64, 1})\n",
    "    \n",
    "# end\n",
    "\n",
    "function conditional(bopt::BayesianOptimization, X::Array{Float64, 2}, y::Array{Float64, 1}, σnoise::Float64)\n",
    "    priorData = bopt.X\n",
    "    posteriorData = X\n",
    "    priorDataCovariates = bopt.y\n",
    "    posteriorDataCovariates = y\n",
    "    \n",
    "    if isnothing(bopt.gp.choleskyFactor) # initialize our model\n",
    "        KXX = covarianceMatrix(bopt.gp.covarianceFunction, priorData, priorData; σnoise=σnoise) # bopt.gp.covarianceFunction.noise instead of σnoise\n",
    "        KXX = Matrix(Hermitian(KXX))\n",
    "        bopt.gp.choleskyFactor = cholesky(KXX).U'\n",
    "    else # update our model\n",
    "        L = bopt.gp.choleskyFactor\n",
    "        KXX = Matrix(Hermitian(L*L'))\n",
    "        KXS = covarianceMatrix(bopt.gp.covarianceFunction, priorData, posteriorData; σnoise=σnoise)\n",
    "        KSS = covarianceMatrix(bopt.gp.covarianceFunction, posteriorData, posteriorData; σnoise=σnoise)\n",
    "        KXplusS = [KXX KXS;\n",
    "                   KXS' KSS]\n",
    "        KXplusS = Matrix(Hermitian(KXplusS))\n",
    "        bopt.gp.choleskyFactor = cholesky(KXplusS).U'\n",
    "        # update bopt with new observations\n",
    "        bopt.X = hcat(bopt.X, X)\n",
    "        bopt.y = vcat(bopt.y, y)\n",
    "        bopt.ybest = maximum(bopt.y)\n",
    "    end\n",
    "    \n",
    "    L = bopt.gp.choleskyFactor\n",
    "    bopt.gp.choleskySolve = L'\\(L\\bopt.y)\n",
    "    \n",
    "    # construct predictive mean function\n",
    "    function μ(xtest)\n",
    "        xtest = toMatrix(xtest)\n",
    "        kxs = covarianceMatrix(bopt.gp.covarianceFunction, bopt.X, xtest; σnoise=σnoise)\n",
    "        return dot(kxs, bopt.gp.choleskySolve)\n",
    "    end\n",
    "    \n",
    "    bopt.gp.predictiveMean = μ\n",
    "    \n",
    "    # construct predictive variance function\n",
    "    function σsquared(xtest)\n",
    "        xtest = toMatrix(xtest)\n",
    "        kxs = covarianceMatrix(bopt.gp.covarianceFunction, bopt.X, xtest; σnoise=σnoise)\n",
    "        v = bopt.gp.choleskyFactor \\ kxs\n",
    "        kss = covarianceMatrix(bopt.gp.covarianceFunction, xtest, xtest; σnoise=σnoise)[1][1]\n",
    "        return kss - dot(v, v)\n",
    "    end\n",
    "    \n",
    "    bopt.gp.predictiveVariance = σsquared\n",
    "    \n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip330\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip330)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip331\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip330)\" d=\"\n",
       "M147.478 1486.45 L2352.76 1486.45 L2352.76 47.2441 L147.478 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip332\">\n",
       "    <rect x=\"147\" y=\"47\" width=\"2206\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip332)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  209.891,1486.45 209.891,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip332)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  903.375,1486.45 903.375,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip332)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1596.86,1486.45 1596.86,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip332)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2290.34,1486.45 2290.34,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  147.478,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  209.891,1486.45 209.891,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  903.375,1486.45 903.375,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1596.86,1486.45 1596.86,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2290.34,1486.45 2290.34,1469.18 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip330)\" d=\"M 0 0 M179.649 1530.29 L209.324 1530.29 L209.324 1534.23 L179.649 1534.23 L179.649 1530.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M220.227 1543.18 L227.866 1543.18 L227.866 1516.82 L219.556 1518.49 L219.556 1514.23 L227.82 1512.56 L232.495 1512.56 L232.495 1543.18 L240.134 1543.18 L240.134 1547.12 L220.227 1547.12 L220.227 1543.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M903.375 1515.64 Q899.764 1515.64 897.935 1519.2 Q896.13 1522.75 896.13 1529.87 Q896.13 1536.98 897.935 1540.55 Q899.764 1544.09 903.375 1544.09 Q907.009 1544.09 908.815 1540.55 Q910.644 1536.98 910.644 1529.87 Q910.644 1522.75 908.815 1519.2 Q907.009 1515.64 903.375 1515.64 M903.375 1511.93 Q909.185 1511.93 912.241 1516.54 Q915.319 1521.12 915.319 1529.87 Q915.319 1538.6 912.241 1543.21 Q909.185 1547.79 903.375 1547.79 Q897.565 1547.79 894.486 1543.21 Q891.431 1538.6 891.431 1529.87 Q891.431 1521.12 894.486 1516.54 Q897.565 1511.93 903.375 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M1587.24 1543.18 L1594.88 1543.18 L1594.88 1516.82 L1586.57 1518.49 L1586.57 1514.23 L1594.83 1512.56 L1599.51 1512.56 L1599.51 1543.18 L1607.15 1543.18 L1607.15 1547.12 L1587.24 1547.12 L1587.24 1543.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2285 1543.18 L2301.31 1543.18 L2301.31 1547.12 L2279.37 1547.12 L2279.37 1543.18 Q2282.03 1540.43 2286.62 1535.8 Q2291.22 1531.15 2292.4 1529.81 Q2294.65 1527.28 2295.53 1525.55 Q2296.43 1523.79 2296.43 1522.1 Q2296.43 1519.34 2294.49 1517.61 Q2292.56 1515.87 2289.46 1515.87 Q2287.26 1515.87 2284.81 1516.63 Q2282.38 1517.4 2279.6 1518.95 L2279.6 1514.23 Q2282.43 1513.09 2284.88 1512.51 Q2287.33 1511.93 2289.37 1511.93 Q2294.74 1511.93 2297.93 1514.62 Q2301.13 1517.31 2301.13 1521.8 Q2301.13 1523.93 2300.32 1525.85 Q2299.53 1527.74 2297.43 1530.34 Q2296.85 1531.01 2293.75 1534.23 Q2290.64 1537.42 2285 1543.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip332)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  147.478,1332.98 2352.76,1332.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip332)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  147.478,981.322 2352.76,981.322 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip332)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  147.478,629.665 2352.76,629.665 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip332)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  147.478,278.007 2352.76,278.007 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  147.478,1486.45 147.478,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  147.478,1332.98 173.941,1332.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  147.478,981.322 173.941,981.322 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  147.478,629.665 173.941,629.665 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  147.478,278.007 173.941,278.007 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip330)\" d=\"M 0 0 M51.3625 1333.43 L81.0383 1333.43 L81.0383 1337.37 L51.3625 1337.37 L51.3625 1333.43 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M95.1586 1346.32 L111.478 1346.32 L111.478 1350.26 L89.5336 1350.26 L89.5336 1346.32 Q92.1956 1343.57 96.7789 1338.94 Q101.385 1334.29 102.566 1332.95 Q104.811 1330.42 105.691 1328.69 Q106.594 1326.93 106.594 1325.24 Q106.594 1322.48 104.649 1320.75 Q102.728 1319.01 99.6261 1319.01 Q97.4271 1319.01 94.9734 1319.77 Q92.5428 1320.54 89.7651 1322.09 L89.7651 1317.37 Q92.5891 1316.23 95.0428 1315.65 Q97.4965 1315.07 99.5335 1315.07 Q104.904 1315.07 108.098 1317.76 Q111.293 1320.45 111.293 1324.94 Q111.293 1327.07 110.483 1328.99 Q109.696 1330.88 107.589 1333.48 Q107.01 1334.15 103.909 1337.37 Q100.807 1340.56 95.1586 1346.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M50.9921 981.774 L80.6679 981.774 L80.6679 985.709 L50.9921 985.709 L50.9921 981.774 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M91.5706 994.667 L99.2095 994.667 L99.2095 968.301 L90.8993 969.968 L90.8993 965.709 L99.1632 964.042 L103.839 964.042 L103.839 994.667 L111.478 994.667 L111.478 998.602 L91.5706 998.602 L91.5706 994.667 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M99.5335 615.463 Q95.9224 615.463 94.0937 619.028 Q92.2882 622.57 92.2882 629.699 Q92.2882 636.806 94.0937 640.371 Q95.9224 643.912 99.5335 643.912 Q103.168 643.912 104.973 640.371 Q106.802 636.806 106.802 629.699 Q106.802 622.57 104.973 619.028 Q103.168 615.463 99.5335 615.463 M99.5335 611.76 Q105.344 611.76 108.399 616.366 Q111.478 620.949 111.478 629.699 Q111.478 638.426 108.399 643.033 Q105.344 647.616 99.5335 647.616 Q93.7234 647.616 90.6447 643.033 Q87.5892 638.426 87.5892 629.699 Q87.5892 620.949 90.6447 616.366 Q93.7234 611.76 99.5335 611.76 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M91.5706 291.352 L99.2095 291.352 L99.2095 264.986 L90.8993 266.653 L90.8993 262.394 L99.1632 260.727 L103.839 260.727 L103.839 291.352 L111.478 291.352 L111.478 295.287 L91.5706 295.287 L91.5706 291.352 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><circle clip-path=\"url(#clip332)\" cx=\"1430.7\" cy=\"923.884\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip332)\" cx=\"1355.25\" cy=\"938.261\" r=\"14\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<path clip-path=\"url(#clip332)\" d=\"\n",
       "M209.891 1383.25 L279.24 1382.13 L348.588 1378.44 L417.937 1371.76 L487.285 1361.72 L556.633 1348.1 L625.982 1330.76 L695.33 1309.75 L764.678 1285.22 L834.027 1257.45 \n",
       "  L903.375 1226.82 L972.723 1193.73 L1042.07 1158.53 L1111.42 1121.48 L1180.77 1082.66 L1250.12 1041.92 L1319.47 998.91 L1388.81 953.118 L1458.16 934.698 L1527.51 960.99 \n",
       "  L1596.86 986.564 L1666.21 1012.14 L1735.56 1038.14 L1804.9 1064.7 L1874.25 1091.69 L1943.6 1118.79 L2012.95 1145.55 L2082.3 1171.46 L2151.65 1196.02 L2220.99 1218.81 \n",
       "  L2290.34 1239.48 L2290.34 173.765 L2220.99 224.314 L2151.65 279.935 L2082.3 339.979 L2012.95 403.566 L1943.6 469.62 L1874.25 536.927 L1804.9 604.214 L1735.56 670.248 \n",
       "  L1666.21 733.932 L1596.86 794.414 L1527.51 851.163 L1458.16 904.009 L1388.81 906.742 L1319.47 876.074 L1250.12 841.706 L1180.77 802.886 L1111.42 759.222 L1042.07 710.752 \n",
       "  L972.723 657.937 L903.375 601.599 L834.027 542.819 L764.678 482.835 L695.33 422.927 L625.982 364.327 L556.633 308.14 L487.285 255.287 L417.937 206.477 L348.588 162.186 \n",
       "  L279.24 122.669 L209.891 87.9763  Z\n",
       "  \" fill=\"#e26f46\" fill-rule=\"evenodd\" fill-opacity=\"0.5\"/>\n",
       "<polyline clip-path=\"url(#clip332)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  209.891,735.613 279.24,752.401 348.588,770.315 417.937,789.117 487.285,808.505 556.633,828.118 625.982,847.545 695.33,866.338 764.678,884.026 834.027,900.136 \n",
       "  903.375,914.211 972.723,925.833 1042.07,934.641 1111.42,940.352 1180.77,942.773 1250.12,941.814 1319.47,937.492 1388.81,929.93 1458.16,919.353 1527.51,906.076 \n",
       "  1596.86,890.489 1666.21,873.035 1735.56,854.194 1804.9,834.458 1874.25,814.31 1943.6,794.205 2012.95,774.556 2082.3,755.717 2151.65,737.98 2220.99,721.564 \n",
       "  2290.34,706.623 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip332)\" style=\"stroke:#3da44d; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  209.891,1177.86 279.24,985.76 348.588,814.122 417.937,670.735 487.285,561.497 556.633,489.882 625.982,456.635 695.33,459.699 764.678,494.402 834.027,553.875 \n",
       "  903.375,629.665 972.723,712.487 1042.07,793.06 1111.42,862.929 1180.77,915.224 1250.12,945.275 1319.47,951.026 1388.81,933.219 1458.16,895.329 1527.51,843.254 \n",
       "  1596.86,784.788 1666.21,728.921 1735.56,685.043 1804.9,662.099 1874.25,667.793 1943.6,707.898 2012.95,785.743 2082.3,901.913 2151.65,1054.2 2220.99,1237.79 \n",
       "  2290.34,1445.72 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip330)\" d=\"\n",
       "M1770.17 337.138 L2279.25 337.138 L2279.25 95.2176 L1770.17 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip330)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1770.17,337.138 2279.25,337.138 2279.25,95.2176 1770.17,95.2176 1770.17,337.138 \n",
       "  \"/>\n",
       "<circle clip-path=\"url(#clip330)\" cx=\"1868.18\" cy=\"155.698\" r=\"23\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"5.12\"/>\n",
       "<path clip-path=\"url(#clip330)\" d=\"M 0 0 M1980.03 175.385 Q1978.23 180.015 1976.51 181.427 Q1974.8 182.839 1971.93 182.839 L1968.53 182.839 L1968.53 179.274 L1971.03 179.274 Q1972.79 179.274 1973.76 178.44 Q1974.73 177.607 1975.91 174.505 L1976.68 172.561 L1966.19 147.052 L1970.7 147.052 L1978.81 167.329 L1986.91 147.052 L1991.42 147.052 L1980.03 175.385 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M1998.71 169.042 L2006.35 169.042 L2006.35 142.677 L1998.04 144.343 L1998.04 140.084 L2006.31 138.418 L2010.98 138.418 L2010.98 169.042 L2018.62 169.042 L2018.62 172.978 L1998.71 172.978 L1998.71 169.042 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip330)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1794.67,216.178 1941.69,216.178 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip330)\" d=\"M 0 0 M1966.19 198.898 L1973.16 198.898 L1981.98 222.416 L1990.84 198.898 L1997.81 198.898 L1997.81 233.458 L1993.25 233.458 L1993.25 203.11 L1984.34 226.814 L1979.64 226.814 L1970.73 203.11 L1970.73 233.458 L1966.19 233.458 L1966.19 198.898 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2029.08 219.43 L2029.08 221.513 L2009.5 221.513 Q2009.78 225.911 2012.14 228.226 Q2014.52 230.518 2018.76 230.518 Q2021.21 230.518 2023.51 229.916 Q2025.82 229.314 2028.09 228.11 L2028.09 232.138 Q2025.8 233.11 2023.39 233.62 Q2020.98 234.129 2018.51 234.129 Q2012.3 234.129 2008.67 230.518 Q2005.06 226.907 2005.06 220.749 Q2005.06 214.384 2008.48 210.657 Q2011.93 206.907 2017.76 206.907 Q2023 206.907 2026.03 210.286 Q2029.08 213.643 2029.08 219.43 M2024.82 218.18 Q2024.78 214.685 2022.86 212.601 Q2020.96 210.518 2017.81 210.518 Q2014.25 210.518 2012.09 212.532 Q2009.96 214.546 2009.64 218.203 L2024.82 218.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2047.86 220.425 Q2042.7 220.425 2040.7 221.606 Q2038.71 222.786 2038.71 225.634 Q2038.71 227.902 2040.2 229.245 Q2041.7 230.564 2044.27 230.564 Q2047.81 230.564 2049.94 228.064 Q2052.09 225.541 2052.09 221.374 L2052.09 220.425 L2047.86 220.425 M2056.35 218.666 L2056.35 233.458 L2052.09 233.458 L2052.09 229.522 Q2050.63 231.883 2048.46 233.018 Q2046.28 234.129 2043.13 234.129 Q2039.15 234.129 2036.79 231.907 Q2034.45 229.661 2034.45 225.911 Q2034.45 221.536 2037.37 219.314 Q2040.31 217.092 2046.12 217.092 L2052.09 217.092 L2052.09 216.675 Q2052.09 213.735 2050.15 212.138 Q2048.23 210.518 2044.73 210.518 Q2042.51 210.518 2040.4 211.05 Q2038.3 211.583 2036.35 212.647 L2036.35 208.712 Q2038.69 207.81 2040.89 207.37 Q2043.09 206.907 2045.17 206.907 Q2050.8 206.907 2053.57 209.823 Q2056.35 212.74 2056.35 218.666 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2086.68 217.809 L2086.68 233.458 L2082.42 233.458 L2082.42 217.948 Q2082.42 214.268 2080.98 212.439 Q2079.55 210.61 2076.68 210.61 Q2073.23 210.61 2071.24 212.81 Q2069.25 215.009 2069.25 218.805 L2069.25 233.458 L2064.96 233.458 L2064.96 207.532 L2069.25 207.532 L2069.25 211.56 Q2070.77 209.222 2072.83 208.064 Q2074.92 206.907 2077.63 206.907 Q2082.09 206.907 2084.38 209.685 Q2086.68 212.439 2086.68 217.809 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2120.29 210.518 Q2116.86 210.518 2114.87 213.203 Q2112.88 215.865 2112.88 220.518 Q2112.88 225.171 2114.85 227.856 Q2116.84 230.518 2120.29 230.518 Q2123.69 230.518 2125.68 227.833 Q2127.67 225.147 2127.67 220.518 Q2127.67 215.911 2125.68 213.226 Q2123.69 210.518 2120.29 210.518 M2120.29 206.907 Q2125.84 206.907 2129.01 210.518 Q2132.19 214.129 2132.19 220.518 Q2132.19 226.884 2129.01 230.518 Q2125.84 234.129 2120.29 234.129 Q2114.71 234.129 2111.54 230.518 Q2108.39 226.884 2108.39 220.518 Q2108.39 214.129 2111.54 210.518 Q2114.71 206.907 2120.29 206.907 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2152.37 197.439 L2152.37 200.981 L2148.3 200.981 Q2146 200.981 2145.1 201.907 Q2144.22 202.833 2144.22 205.24 L2144.22 207.532 L2151.24 207.532 L2151.24 210.842 L2144.22 210.842 L2144.22 233.458 L2139.94 233.458 L2139.94 210.842 L2135.87 210.842 L2135.87 207.532 L2139.94 207.532 L2139.94 205.726 Q2139.94 201.398 2141.95 199.43 Q2143.97 197.439 2148.34 197.439 L2152.37 197.439 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2194.75 228.527 L2194.75 219.245 L2187.12 219.245 L2187.12 215.402 L2199.38 215.402 L2199.38 230.24 Q2196.68 232.161 2193.41 233.157 Q2190.15 234.129 2186.44 234.129 Q2178.34 234.129 2173.76 229.407 Q2169.2 224.661 2169.2 216.212 Q2169.2 207.74 2173.76 203.018 Q2178.34 198.273 2186.44 198.273 Q2189.82 198.273 2192.86 199.106 Q2195.91 199.939 2198.48 201.56 L2198.48 206.536 Q2195.89 204.337 2192.97 203.226 Q2190.06 202.115 2186.84 202.115 Q2180.5 202.115 2177.3 205.657 Q2174.13 209.198 2174.13 216.212 Q2174.13 223.203 2177.3 226.745 Q2180.5 230.286 2186.84 230.286 Q2189.31 230.286 2191.26 229.87 Q2193.2 229.43 2194.75 228.527 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2212.6 202.74 L2212.6 215.726 L2218.48 215.726 Q2221.75 215.726 2223.53 214.036 Q2225.31 212.347 2225.31 209.222 Q2225.31 206.12 2223.53 204.43 Q2221.75 202.74 2218.48 202.74 L2212.6 202.74 M2207.93 198.898 L2218.48 198.898 Q2224.29 198.898 2227.25 201.536 Q2230.24 204.152 2230.24 209.222 Q2230.24 214.337 2227.25 216.953 Q2224.29 219.569 2218.48 219.569 L2212.6 219.569 L2212.6 233.458 L2207.93 233.458 L2207.93 198.898 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip330)\" style=\"stroke:#3da44d; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1794.67,276.658 1941.69,276.658 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip330)\" d=\"M 0 0 M1982.21 262.549 Q1977.12 262.549 1974.11 266.345 Q1971.12 270.141 1971.12 276.692 Q1971.12 283.22 1974.11 287.016 Q1977.12 290.813 1982.21 290.813 Q1987.3 290.813 1990.26 287.016 Q1993.25 283.22 1993.25 276.692 Q1993.25 270.141 1990.26 266.345 Q1987.3 262.549 1982.21 262.549 M1982.21 258.753 Q1989.48 258.753 1993.83 263.637 Q1998.18 268.498 1998.18 276.692 Q1998.18 284.864 1993.83 289.748 Q1989.48 294.609 1982.21 294.609 Q1974.92 294.609 1970.54 289.748 Q1966.19 284.887 1966.19 276.692 Q1966.19 268.498 1970.54 263.637 Q1974.92 258.753 1982.21 258.753 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2023.92 280.998 Q2023.92 276.299 2021.98 273.637 Q2020.06 270.952 2016.68 270.952 Q2013.3 270.952 2011.35 273.637 Q2009.43 276.299 2009.43 280.998 Q2009.43 285.697 2011.35 288.382 Q2013.3 291.044 2016.68 291.044 Q2020.06 291.044 2021.98 288.382 Q2023.92 285.697 2023.92 280.998 M2009.43 271.947 Q2010.77 269.632 2012.81 268.521 Q2014.87 267.387 2017.72 267.387 Q2022.44 267.387 2025.38 271.137 Q2028.34 274.887 2028.34 280.998 Q2028.34 287.109 2025.38 290.859 Q2022.44 294.609 2017.72 294.609 Q2014.87 294.609 2012.81 293.498 Q2010.77 292.363 2009.43 290.049 L2009.43 293.938 L2005.15 293.938 L2005.15 257.919 L2009.43 257.919 L2009.43 271.947 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2035.4 268.012 L2039.66 268.012 L2039.66 294.401 Q2039.66 299.354 2037.76 301.576 Q2035.89 303.799 2031.7 303.799 L2030.08 303.799 L2030.08 300.188 L2031.21 300.188 Q2033.64 300.188 2034.52 299.053 Q2035.4 297.942 2035.4 294.401 L2035.4 268.012 M2035.4 257.919 L2039.66 257.919 L2039.66 263.313 L2035.4 263.313 L2035.4 257.919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2070.75 279.91 L2070.75 281.993 L2051.17 281.993 Q2051.45 286.391 2053.81 288.706 Q2056.19 290.998 2060.43 290.998 Q2062.88 290.998 2065.17 290.396 Q2067.49 289.794 2069.76 288.59 L2069.76 292.618 Q2067.46 293.59 2065.06 294.1 Q2062.65 294.609 2060.17 294.609 Q2053.97 294.609 2050.33 290.998 Q2046.72 287.387 2046.72 281.229 Q2046.72 274.864 2050.15 271.137 Q2053.6 267.387 2059.43 267.387 Q2064.66 267.387 2067.7 270.766 Q2070.75 274.123 2070.75 279.91 M2066.49 278.66 Q2066.45 275.165 2064.52 273.081 Q2062.63 270.998 2059.48 270.998 Q2055.91 270.998 2053.76 273.012 Q2051.63 275.026 2051.31 278.683 L2066.49 278.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2096.4 269.007 L2096.4 272.989 Q2094.59 271.993 2092.76 271.507 Q2090.96 270.998 2089.11 270.998 Q2084.96 270.998 2082.67 273.637 Q2080.38 276.252 2080.38 280.998 Q2080.38 285.743 2082.67 288.382 Q2084.96 290.998 2089.11 290.998 Q2090.96 290.998 2092.76 290.512 Q2094.59 290.002 2096.4 289.007 L2096.4 292.942 Q2094.62 293.776 2092.69 294.192 Q2090.8 294.609 2088.64 294.609 Q2082.79 294.609 2079.34 290.928 Q2075.89 287.248 2075.89 280.998 Q2075.89 274.655 2079.36 271.021 Q2082.86 267.387 2088.92 267.387 Q2090.89 267.387 2092.76 267.803 Q2094.64 268.197 2096.4 269.007 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2108.02 260.651 L2108.02 268.012 L2116.79 268.012 L2116.79 271.322 L2108.02 271.322 L2108.02 285.396 Q2108.02 288.567 2108.88 289.47 Q2109.75 290.373 2112.42 290.373 L2116.79 290.373 L2116.79 293.938 L2112.42 293.938 Q2107.49 293.938 2105.61 292.109 Q2103.74 290.257 2103.74 285.396 L2103.74 271.322 L2100.61 271.322 L2100.61 268.012 L2103.74 268.012 L2103.74 260.651 L2108.02 260.651 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2122.39 268.012 L2126.65 268.012 L2126.65 293.938 L2122.39 293.938 L2122.39 268.012 M2122.39 257.919 L2126.65 257.919 L2126.65 263.313 L2122.39 263.313 L2122.39 257.919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2132.51 268.012 L2137.02 268.012 L2145.13 289.771 L2153.23 268.012 L2157.74 268.012 L2148.02 293.938 L2142.23 293.938 L2132.51 268.012 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip330)\" d=\"M 0 0 M2185.8 279.91 L2185.8 281.993 L2166.21 281.993 Q2166.49 286.391 2168.85 288.706 Q2171.24 290.998 2175.47 290.998 Q2177.93 290.998 2180.22 290.396 Q2182.53 289.794 2184.8 288.59 L2184.8 292.618 Q2182.51 293.59 2180.1 294.1 Q2177.69 294.609 2175.22 294.609 Q2169.01 294.609 2165.38 290.998 Q2161.77 287.387 2161.77 281.229 Q2161.77 274.864 2165.19 271.137 Q2168.64 267.387 2174.48 267.387 Q2179.71 267.387 2182.74 270.766 Q2185.8 274.123 2185.8 279.91 M2181.54 278.66 Q2181.49 275.165 2179.57 273.081 Q2177.67 270.998 2174.52 270.998 Q2170.96 270.998 2168.81 273.012 Q2166.68 275.026 2166.35 278.683 L2181.54 278.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain = -1.:.1:2.\n",
    "zero_mean(x) = 0.\n",
    "sekernel2 = SE()\n",
    "# sekernel2 = SquaredExponential(0.1, 5.0, 0.1)\n",
    "gp2 = GP(zero_mean, sekernel2)\n",
    "bopt2 = BayesianOptimization(gp2, X, y)\n",
    "conditional(bopt2, X, y, σnoise)\n",
    "\n",
    "scatter(bopt2.X', bopt2.y)\n",
    "plot!(\n",
    "    domain,\n",
    "    bopt2.gp.predictiveMean.(domain),\n",
    "    ribbon=2*sqrt.(bopt2.gp.predictiveVariance.(domain)),\n",
    "    label=\"Mean of GP\"\n",
    ")\n",
    "plot!(domain, blackBoxProcess.(domain; σnoise=0.0), label=\"Objective\") # true objective\n",
    "# bopt.gp.choleskySolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeking an Optimal Value via Expected Improvement\n",
    "Thus far, we've been able to abstract away enough information about bayesian optimization in order to give us a relatively\n",
    "seamless handle on computations of interest. Now, we turn our attention to minimizing our objective function via\n",
    "expected improvement. As a brief aside, we'll derive the analytic representation of expected improvement below:\n",
    "\n",
    "Mathematically, our improvement at x can be expressed as follows: $I(x) = max(Y - f^{*}, 0)$. Where\n",
    "$Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is the random variable that corresponds to the function value at x. Since $I$ is a random\n",
    "variable, one can consider the average (expected) improvement (EI) to assess x.\n",
    "\n",
    "$$\n",
    "EI(x) = \\mathbb{E}_{Y \\sim \\mathcal{N}(\\mu, \\sigma^2)}\\left[ I(x) \\right]\n",
    "$$\n",
    "\n",
    "With the reparameterization trick, $Y = \\mu + \\sigma \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, 1)$, we have:\n",
    "\n",
    "$$\n",
    "EI(x) = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, 1)}\\left[ I(x) \\right]\n",
    "$$\n",
    "\n",
    "which can be written as\n",
    "\n",
    "$$\n",
    "EI(x) = \\int_{-\\infty}^{\\infty} I(x) \\phi(\\epsilon) d\\epsilon\n",
    "$$\n",
    "\n",
    "This integral only has a value greater than 0 when $I(x) > 0$. \n",
    "$$\n",
    "\\therefore I(x) > 0 \\Rightarrow EI(x) > 0 \\\\\n",
    "\\Updownarrow \\\\\n",
    "\\mu + \\sigma \\epsilon - f^{*} > 0 \\Rightarrow EI(x) > 0 \\\\\n",
    "\\Updownarrow \\\\\n",
    "\\epsilon > \\frac{f^{*} - \\mu}{\\sigma} \\Rightarrow EI(x) > 0\n",
    "$$\n",
    "\n",
    "Now we solve for $EI(x)$.\n",
    "\n",
    "\\begin{align*}\n",
    "EI(x) &= \\int_{\\frac{f^{*}-\\mu}{\\sigma}}^{\\infty} (\\mu + \\sigma \\epsilon - f^{*}) \\phi(\\epsilon) d\\epsilon\\\\\n",
    "&= (\\mu - f^{*}) \\left[ 1 - \\Phi(\\frac{f^{*} - \\mu}{\\sigma}) \\right] + \\sigma \\phi(\\frac{f^{*} - \\mu}{\\sigma})\n",
    "\\end{align*}\n",
    "\n",
    "But, $EI(x)$ is piecewise, therefore we have the following:\n",
    "\n",
    "$$\n",
    "EI(\\textbf{x}) =\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      (\\mu(\\textbf{x}) - f(\\textbf{x}^*) - \\xi)\\Phi(Z) + \\sigma(\\textbf{x})\\phi(Z) & \\sigma(\\textbf{x}) \\gt 0 \\\\\n",
    "      0 & \\sigma(\\textbf{x}) = 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "Z =\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\frac{\\mu(\\textbf{x}) - f(\\textbf{x}^*) - \\xi}{\\sigma(\\textbf{x})} & \\sigma(\\textbf{x}) \\gt 0 \\\\\n",
    "      0 & \\sigma(\\textbf{x}) = 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ei (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function z(x, bopt::BayesianOptimization)\n",
    "    σ̂(xa) = sqrt(abs(bopt.gp.predictiveVariance(xa)))\n",
    "    μ̂(xa) = bopt.gp.predictiveMean(xa)\n",
    "    f⁺ = bopt.ybest\n",
    "    ξ = .1\n",
    "    return σ̂(x) > 0 ? (μ̂(x) - f⁺ - ξ) / σ̂(x) : 0\n",
    "end\n",
    "\n",
    "function ei(x, bopt::BayesianOptimization)\n",
    "    z_eval = z(x, bopt)\n",
    "    normal = Normal()\n",
    "    normal_cdf_at_z = cdf(normal, z_eval)\n",
    "    normal_pdf_at_z = pdf(normal, z_eval)\n",
    "    σ̂(xa) = abs(sqrt(bopt.gp.predictiveVariance(xa)))\n",
    "    μ̂(xa) = bopt.gp.predictiveMean(xa)\n",
    "    f⁺ = bopt.ybest\n",
    "    ξ = .1\n",
    "    \n",
    "    return σ̂(x) > 0 ? (μ̂(x) - f⁺ - ξ) * normal_cdf_at_z + σ̂(x)*normal_pdf_at_z : 0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization w/Expected Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively build our plots while conditioning on new data\n",
    "\n",
    "# Setup\n",
    "# Suppose we have some process and we have n known function evaluations\n",
    "n = 2\n",
    "d = 1\n",
    "σnoise = 0.05\n",
    "domain = -1.:.1:2.\n",
    "\n",
    "# Variates and Covariates for initialization\n",
    "X = round.(\n",
    "    rand(Uniform(domain[1], domain[end]), d, n), \n",
    "    digits=4\n",
    ") \n",
    "y = vec(blackBoxProcess.(X; σnoise=σnoise))\n",
    "\n",
    "# Initialize our models\n",
    "zero_mean(x) = 0.\n",
    "sekernel = SE()\n",
    "gp = GP(zero_mean, sekernel)\n",
    "bopt = BayesianOptimization(gp, X, y)\n",
    "conditional(bopt, X, y, σnoise)\n",
    "\n",
    "# Keep track of our plots\n",
    "obsAndObjAndGpPlots = [plot(domain, blackBoxProcess.(domain; σnoise=0.0), label=\"Objective\", legend=:outertopleft)]\n",
    "plot!(obsAndObjAndGpPlots[1], bopt.X', bopt.y, seriestype=:scatter)\n",
    "plot!(\n",
    "    obsAndObjAndGpPlots[1],\n",
    "    domain,\n",
    "    bopt.gp.predictiveMean.(domain),\n",
    "    ribbon=2*sqrt.(abs.(bopt.gp.predictiveVariance.(domain))),\n",
    "    label=\"μ ± 2σ (GP)\"\n",
    ");\n",
    "\n",
    "# A few iterations of BO\n",
    "BUDGET = 20\n",
    "for budget = 1:BUDGET\n",
    "    # Find new sample by solving ∇EI = 0\n",
    "    einow(x) = ei(x, bopt)\n",
    "    result = optimize(x -> -einow(first(x)), domain[1], domain[end])\n",
    "    maximizer = [Optim.minimizer(result)]\n",
    "    \n",
    "    # Sample based on recommendation\n",
    "    Xnew = reshape(maximizer, (1, 1))\n",
    "    ynew = vec(blackBoxProcess.(Xnew; σnoise=σnoise))\n",
    "    \n",
    "    # Update model\n",
    "    conditional(bopt, Xnew, ynew, σnoise)\n",
    "    \n",
    "    # Append plot\n",
    "    push!(obsAndObjAndGpPlots, plot(domain, blackBoxProcess.(domain; σnoise=0.0), label=\"Objective\", legend=:outertopleft))\n",
    "    plot!(obsAndObjAndGpPlots[budget+1], bopt.X', bopt.y, seriestype=:scatter)\n",
    "    plot!(\n",
    "        obsAndObjAndGpPlots[budget+1],\n",
    "        domain,\n",
    "        bopt.gp.predictiveMean.(domain),\n",
    "        ribbon=2sqrt.(abs.(bopt.gp.predictiveVariance.(domain))),\n",
    "        label=\"μ ± 2σ (GP)\",\n",
    "    )    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i = 1:BUDGET\n",
    "    savefig(obsAndObjAndGpPlots[i], \"model0$(i).png\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: bopt not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: bopt not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[1]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "bopt.ybest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottom Up Computations for BO w/EI\n",
    "Everything below here follows from the derivative computations from our other notebook with minor modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "δ∇z (generic function with 1 method)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@kwdef mutable struct GaussianKernel <: Kernel\n",
    "    ν²::Float64 # provided as ν²\n",
    "    ℓ::Float64 # provided as l\n",
    "    σn::Float64 # provided as σ\n",
    "    \n",
    "    function GaussianKernel(ν², ℓ, σn)\n",
    "        @assert ν² >= 0 \"variance must be a positive real number\"\n",
    "        \n",
    "        return new(ν², ℓ, σn)\n",
    "    end\n",
    "end\n",
    "\n",
    "GaussianKernel() = GaussianKernel(ν²=1.0, ℓ=1.0, σn=0.0)\n",
    "GK() = GaussianKernel()\n",
    "\n",
    "function (gk::GaussianKernel)(ρ::Float64)::Float64\n",
    "    δxx = Int8(ρ == 0)\n",
    "    return gk.ν² * exp(-ρ^2 / (2*gk.ℓ^2)) + δxx*gk.σn^2\n",
    "end\n",
    "\n",
    "# After your kernel is constructed, we can layer on our computations for \n",
    "# derivatives w.r.t. data and hyperparameters as follows\n",
    "gk = GaussianKernel(ℓ=1.01, ν²=1., σn=0.)\n",
    "\n",
    "# ψ Derivatives and Perturbations\n",
    "ψ(ρ) = gk(ρ)\n",
    "dψ(ρ) = ψ(ρ) * (-ρ/(gk.ℓ^2))\n",
    "d2ψ(ρ) = (ψ(ρ)/gk.ℓ^2)*(ρ^2/gk.ℓ^2 - 1)\n",
    "δψ(ρ) = ψ(ρ) * (ρ^2/gk.ℓ^3)\n",
    "δdψ(ρ) = ψ(ρ)*(-ρ/gk.ℓ^3) * (ρ^2/gk.ℓ^2 - 2)\n",
    "\n",
    "# k(x, y) Derivatives and Perturbations\n",
    "k(x, y) = ψ(norm(x-y))\n",
    "\n",
    "function ∇k(x, y)\n",
    "    ρ = norm(x-y)\n",
    "    return dψ(ρ) * (x-y)/ρ\n",
    "end\n",
    "\n",
    "function Hk(x, y)\n",
    "    r = x-y\n",
    "    ρ = norm(r)\n",
    "    return (d2ψ(ρ) - dψ(ρ)/ρ)/ρ^2 * r * r' + dψ(ρ)/ρ * I\n",
    "end\n",
    "\n",
    "δk(x, y) = δψ(norm(x-y))\n",
    "\n",
    "function δ∇k(x, y)\n",
    "    ρ = norm(x-y)\n",
    "    return δdψ(ρ) * (x-y)/ρ\n",
    "end\n",
    "\n",
    "# Kernel Matrix builder wrt Covariance Function\n",
    "function kernel_matrix(X::AbstractMatrix, Y::AbstractMatrix; kernel=k)\n",
    "    @assert isCallable(kernel) == true\n",
    "    n = size(X)[2]\n",
    "    m = size(Y)[2]\n",
    "    K = zeros(n, m)\n",
    "    for i = 1:n\n",
    "        for j = 1:m\n",
    "            K[i,j] = k(X[:,i], Y[:,j])\n",
    "        end\n",
    "    end\n",
    "    return K\n",
    "end\n",
    "\n",
    "# μ Derivatives and Perturbations\n",
    "function μ(x, X, c)\n",
    "    μx = 0.0\n",
    "    for i = 1:size(X)[2]\n",
    "        μx += c[i] * k(x, X[:,i])\n",
    "    end\n",
    "    return μx\n",
    "end\n",
    "\n",
    "function ∇μ(x, X, c)\n",
    "    ∇μx = zeros(length(x))\n",
    "    for i = 1:size(X)[2]\n",
    "        ∇μx += c[i] * ∇k(x, X[:,i])\n",
    "    end\n",
    "    return ∇μx\n",
    "end\n",
    "\n",
    "function Hμ(x, X, c)\n",
    "    Hμx = zeros(length(x), length(x))\n",
    "    for i = 1:size(X)[2]\n",
    "        Hμx += c[i] * Hk(x, X[:,i])\n",
    "    end\n",
    "    return Hμx\n",
    "end\n",
    "\n",
    "function δμ(x, X, c, ẏ, l̇)\n",
    "    δμx = 0.0\n",
    "    KXX = kernel_matrix(X, X)\n",
    "    KXx = kernel_matrix(X, reshape(x, 2, 1))\n",
    "    d = KXX \\ KXx\n",
    "    \n",
    "    for i = 1:size(X)[2]\n",
    "        δμx += c[i] * δk(x, X[:, i]) * l̇ + d[i]*ẏ[i]\n",
    "        for j = 1:size(X)[2]\n",
    "            δμx -= d[i] * δk(X[:, i], X[:, j]) * l̇ * c[j] \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return δμx\n",
    "end\n",
    "\n",
    "function δ∇μ(x, X, c, ẏ, l̇)\n",
    "    δ∇μx = zeros(length(x))\n",
    "    W = zeros(size(X))\n",
    "    \n",
    "    for ndx = 1:size(X)[2]\n",
    "        W[:, ndx] = ∇k(x, X[:, ndx])\n",
    "        δ∇μx += δ∇k(x, X[:, ndx]) * c[ndx] * l̇\n",
    "    end\n",
    "    \n",
    "    W /= kernel_matrix(X, X)\n",
    "    z = copy(ẏ)\n",
    "    \n",
    "    for i = 1:size(X)[2]\n",
    "        for j = 1:size(X)[2]\n",
    "            z[i] -= δk(X[:, i], X[:, j]) * l̇ * c[j]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    δ∇μx += W*z\n",
    "    \n",
    "    return δ∇μx\n",
    "end\n",
    "\n",
    "# σ Derivatives and Perturbations\n",
    "function Hσ(x, X; ℓ=1.0, σref=1.0)\n",
    "    Hσx = zeros(length(x), length(x))\n",
    "    KXx = kernel_matrix(X, reshape(x, length(x), 1))\n",
    "    d = kernel_matrix(X, X) \\ KXx\n",
    "    \n",
    "    W = zeros(size(X))\n",
    "    for col = 1:size(W)[2]\n",
    "       W[:, col] = ∇k(x, X[:, col]) \n",
    "    end\n",
    "    W /= kernel_matrix(X, X, ℓ=ℓ, σref=σref)\n",
    "    \n",
    "    for i = 1:size(X)[2]\n",
    "        Hσx += Hk(x, X[:, i])*d[i] + ∇k(x, X[:, i])*W[:, i]'\n",
    "    end\n",
    "    \n",
    "    Hσx += ∇σ(x, X)*∇σ(x, X)'\n",
    "    Hσx ./= -σ(x, X)\n",
    "    \n",
    "    return Hσx\n",
    "end\n",
    "\n",
    "function δ∇σ(x, X, l̇)\n",
    "    δ∇σx = δσ(x, X, l̇) * ∇σ(x, X)\n",
    "    KXX = kernel_matrix(X, X)\n",
    "    KXx = kernel_matrix(X, reshape(x, length(x), 1))\n",
    "    d = KXX \\ KXx\n",
    "    \n",
    "    W = zeros(size(X))\n",
    "    for ndx = 1:size(X)[2]\n",
    "        W[:, ndx] = ∇k(x, X[:, ndx])\n",
    "    end\n",
    "    W /= kernel_matrix(X, X)\n",
    "    \n",
    "    z0 = zeros(length(x))\n",
    "    z1 = zeros(size(X)[2])\n",
    "    z2 = zeros(size(X)[2])\n",
    "    \n",
    "    for i = 1:size(X)[2]\n",
    "        z0 += δ∇k(x, X[:, i]) * d[i] * l̇\n",
    "        z2[i] = δk(x, X[:, i]) * l̇\n",
    "        for j = 1:size(X)[2]\n",
    "            z1[i] += δk(X[:, i], X[:, j]) * d[j] * l̇\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    δ∇σx += -W*z1 + W*z2 + z0\n",
    "    δ∇σx /= -σ(x, X)\n",
    "\n",
    "    return δ∇σx\n",
    "end\n",
    "\n",
    "function δσ(x, X, l̇)\n",
    "    δσx = δk(x, x) * l̇\n",
    "    KXX = kernel_matrix(X, X)\n",
    "    KXx = kernel_matrix(X, reshape(x, 2, 1))\n",
    "    d = KXX \\ KXx\n",
    "    \n",
    "    for i = 1:size(X)[2]\n",
    "        δσx -= 2δk(x, X[:, i]) * d[i] * l̇\n",
    "        for j = 1:size(X)[2]\n",
    "            δσx += d[i] * d[j] * δk(X[:, i], X[:, j]) * l̇\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    δσx /= 2σ(x, X)\n",
    "    return δσx\n",
    "end\n",
    "\n",
    "function σ(x, X)\n",
    "    KXX = kernel_matrix(X, X)\n",
    "    KXx = kernel_matrix(X, reshape(x, length(x), 1))\n",
    "    return √(k(x, x) - dot(KXx, KXX \\ KXx))\n",
    "end\n",
    "\n",
    "function ∇σ(x, X; ℓ=1.0, σref=1.0)\n",
    "    ∇σx = zeros(length(x))\n",
    "    KXx = kernel_matrix(X, reshape(x, length(x), 1))\n",
    "    d = kernel_matrix(X, X) \\ KXx\n",
    "    \n",
    "    for i = 1:size(X)[2]\n",
    "        ∇σx += d[i] * ∇k(x, X[:, i])\n",
    "    end\n",
    "    \n",
    "    ∇σx /= -σ(x, X)\n",
    "    \n",
    "    return ∇σx\n",
    "end\n",
    "\n",
    "# z Derivatives and Perturbations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finite difference check on dψ:  2.4783983521883724e-9\n",
      "Finite difference check on d2ψ: -3.4950177808179996e-9\n",
      "Finite difference check on δψ: -1.3911351909046007e-9\n",
      "Finite difference check on δdψ: 2.6916836756405967e-9\n"
     ]
    }
   ],
   "source": [
    "# Derivatives of spatial difference norm\n",
    "ψ(ρ) = gk(ρ)\n",
    "dψ(ρ) = ψ(ρ) * (-ρ/(gk.ℓ^2))\n",
    "# d2ψ(ρ) = -dψ(ρ)*(ρ/gk.ℓ^2) - ψ(ρ)*(1/gk.ℓ^2)\n",
    "d2ψ(ρ) = (ψ(ρ)/gk.ℓ^2)*(ρ^2/gk.ℓ^2 - 1)\n",
    "\n",
    "# Perturbations to hypers\n",
    "δψ(ρ) = ψ(ρ) * (ρ^2/gk.ℓ^3)\n",
    "δdψ(ρ) = ψ(ρ)*(-ρ/gk.ℓ^3) * (ρ^2/gk.ℓ^2 - 2)\n",
    "\n",
    "ρ = 1.23\n",
    "h = 1e-4\n",
    "fd_dψρ = ( ψ(ρ+h)-ψ(ρ-h) )/(2h)\n",
    "fd_d2ψρ = ( ψ(ρ+h)-2*ψ(ρ)+ψ(ρ-h) )/h^2\n",
    "relerr_dψρ = (dψ(ρ) - fd_dψρ)/dψ(ρ)\n",
    "relerr_d2ψρ = (d2ψ(ρ) - fd_d2ψρ)/d2ψ(ρ)\n",
    "\n",
    "println(\"Finite difference check on dψ:  $relerr_dψρ\")\n",
    "println(\"Finite difference check on d2ψ: $relerr_d2ψρ\")\n",
    "\n",
    "gkplush = GaussianKernel(ℓ=1.01+h, ν²=1., σn=0.)\n",
    "gkminush = GaussianKernel(ℓ=1.01-h, ν²=1., σn=0.)\n",
    "ψplush(ρ) = gkplush(ρ)\n",
    "ψminush(ρ) = gkminush(ρ)\n",
    "\n",
    "fd_δψ = (ψplush(ρ) - ψminush(ρ))/(2h) # finite difference of perturbation to hypers\n",
    "\n",
    "# Verify mixed derivatives by perturbing hypers of the spatial derivative\n",
    "δψplush(ρ) = ψ(ρ) * (ρ^2/gk.ℓ^3)\n",
    "δψminush(ρ) = ψ(ρ) * (ρ^2/gk.ℓ^3)\n",
    "\n",
    "fd_δdψ = (δψplush(ρ+h) - δψminush(ρ-h))/(2h)\n",
    "# fd_δdψ = (dψ(ρ; ℓ=1.3+h)-dψ(ρ; ℓ=1.3-h))/(2h)\n",
    "\n",
    "println(\"Finite difference check on δψ: $( (δψ(ρ)-fd_δψ)/δψ(ρ) )\")\n",
    "println(\"Finite difference check on δdψ: $( (δdψ(ρ)-fd_δdψ)/δdψ(ρ) )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check on derivative: 2.0048209401392056e-9\n",
      "Check second derivative: -3.417082552997685e-8\n",
      "Check second derivative (higher-order approx): -7.286854021791007e-9\n",
      "Absolute Difference: 6.938893903907228e-9\n"
     ]
    }
   ],
   "source": [
    "k(x, y) = ψ(norm(x-y))\n",
    "\n",
    "function ∇k(x, y)\n",
    "    ρ = norm(x-y)\n",
    "    return dψ(ρ) * (x-y)/ρ\n",
    "end\n",
    "\n",
    "function Hk(x, y)\n",
    "    r = x-y\n",
    "    ρ = norm(r)\n",
    "    return (d2ψ(ρ) - dψ(ρ)/ρ)/ρ^2 * r * r' + dψ(ρ)/ρ * I\n",
    "end\n",
    "\n",
    "δk(x, y) = δψ(norm(x-y))\n",
    "\n",
    "function δ∇k(x, y)\n",
    "    ρ = norm(x-y)\n",
    "    return δdψ(ρ) * (x-y)/ρ\n",
    "end\n",
    "\n",
    "x = rand(2)\n",
    "y = rand(2)\n",
    "u = rand(2)\n",
    "\n",
    "dk_du = ∇k(x, y)'*u\n",
    "fd_dk_du = (k(x+h*u, y)-k(x-h*u, y))/(2h)\n",
    "println(\"Check on derivative: $( (dk_du-fd_dk_du)/dk_du )\")\n",
    "\n",
    "d2k_du2 = u'*Hk(x, y)*u\n",
    "fd_d2k_du2 = ( k(x+h*u, y)-2*k(x, y)+k(x-h*u, y) )/h^2\n",
    "alt_fd_d2k_du2 = ( k(x-2h*u, y) + 4k(x-h*u, y) - 10k(x, y) + 4k(x+h*u, y) + k(x+2h*u, y) ) / 8h^2\n",
    "println(\"Check second derivative: $( (d2k_du2-fd_d2k_du2)/d2k_du2 )\")\n",
    "println(\"Check second derivative (higher-order approx): $(( d2k_du2-alt_fd_d2k_du2) / d2k_du2)\")\n",
    "println(\"Absolute Difference: $(abs(alt_fd_d2k_du2 - fd_d2k_du2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean derivatives\n",
    "\n",
    "Let $K_{XX}$ denote the kernel matrix, and $k_{Xx}$ the column vector of kernel evaluations at $x$.\n",
    "The posterior mean function for the GP (assuming a zero-mean prior) is\n",
    "$$\n",
    "  \\mu = k_{xX} c\n",
    "$$\n",
    "where $K_{XX} c = y$.  Note that $c$ does not depend on $x$, but it does depend on the data and hyperparameters.\n",
    "\n",
    "Differentiating in space is straightforward, as we only invoke the kernel derivatives:\n",
    "$$\\begin{aligned}\n",
    "  \\mu_{,i} &= k_{xX,i} c \\\\\n",
    "  \\mu_{,ij} &= k_{xX,ij} c\n",
    "\\end{aligned}$$\n",
    "Differentiating in the data and hyperparameters requires that we also differentiate through a matrix solve:\n",
    "$$\n",
    "  \\dot{\\mu} = \\dot{k}_{xX} K_{XX}^{-1} y + k_{xX} K_{XX}^{-1} \\dot{y} - k_{xX} K_{XX}^{-1} \\dot{K}_{XX} K_{XX}^{-1} y.\n",
    "$$\n",
    "Defining $d = K_{XX}^{-1} k_{Xx}$, we have\n",
    "$$\n",
    "  \\dot{\\mu} = \\dot{k}_{xX} c + d^T (\\dot{y} - \\dot{K}_{XX} c).\n",
    "$$\n",
    "Now differentiating in space and defining $K_{XX}^{-1} k_{Xx,i}$ as $w^{(i)}$, we have\n",
    "$$\n",
    "  \\dot{\\mu}_{,i} = \\dot{k}_{xX,i} c + (w^{(i)})^T (\\dot{y} - \\dot{K}_{XX} c).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "μ([0.3466903879162142, 0.38831428364295983]) = 1.097061779952071 ≈ 1.1233189552021339]\n",
      "∇μ([0.3466903879162142, 0.38831428364295983]) = [0.9191501774196098, 2.03438656227938] ≈ [1, 2]\n",
      "Hμ([0.3466903879162142, 0.38831428364295983]) = [0.28000314640340207 0.08988595858233417; 0.08988595858233417 0.24643083516690467] ≈ 0 matrix\n",
      "Finite difference check on Hμ: -6.363622607553539e-6\n",
      "Finite difference checn on Hμ again: -1.4470493251450963e-8\n"
     ]
    }
   ],
   "source": [
    "function kernel_matrix(kernel, X, Y)\n",
    "    n = size(X)[2]\n",
    "    m = size(Y)[2]\n",
    "    K = zeros(n, m)\n",
    "    for i = 1:n\n",
    "        for j = 1:m\n",
    "            K[i,j] = kernel(X[:,i], Y[:,j])\n",
    "        end\n",
    "    end\n",
    "    return K\n",
    "end\n",
    "\n",
    "function μ(k, x, X, c)\n",
    "    μx = 0.0\n",
    "    for i = 1:size(X)[2]\n",
    "        μx += c[i] * k(x, X[:,i])\n",
    "    end\n",
    "    return μx\n",
    "end\n",
    "\n",
    "function ∇μ(x, X, c)\n",
    "    ∇μx = zeros(length(x))\n",
    "    for i = 1:size(X)[2]\n",
    "        ∇μx += c[i] * ∇k(x, X[:,i])\n",
    "    end\n",
    "    return ∇μx\n",
    "end\n",
    "\n",
    "function Hμ(x, X, c)\n",
    "    Hμx = zeros(length(x), length(x))\n",
    "    for i = 1:size(X)[2]\n",
    "        Hμx += c[i] * Hk(x, X[:,i])\n",
    "    end\n",
    "    return Hμx\n",
    "end\n",
    "\n",
    "# Set up a test problem\n",
    "\n",
    "X = rand(2, 10)\n",
    "y = X[1,:] + 2*X[2,:]\n",
    "KXX = kernel_matrix(k, X, X)\n",
    "c = KXX\\y\n",
    "\n",
    "x = rand(2)\n",
    "println(\"μ($x) = $(μ(k, x, X, c)) ≈ $(x[1] + 2*x[2])]\")\n",
    "println(\"∇μ($x) = $(∇μ(x, X, c)) ≈ [1, 2]\")\n",
    "println(\"Hμ($x) = $(Hμ(x, X, c)) ≈ 0 matrix\")\n",
    "\n",
    "u = rand(2)\n",
    "d2μ_du2 = u'*Hμ(x, X, c)*u\n",
    "fd_d2μ_du2 = (μ(x+2h*u, X, c) - 2μ(x, X, c) + μ(x-2h*u, X, c)) / 4h^2\n",
    "relerr_Hμ = (d2μ_du2 - fd_d2μ_du2) / d2μ_du2\n",
    "println(\"Finite difference check on Hμ: $(relerr_Hμ)\")\n",
    "\n",
    "# Finite difference check for Hμ\n",
    "fd_∇μ_du = u'*( ∇μ(x+h*u, X, c) - ∇μ(x-h*u, X, c) ) / (2h)\n",
    "relerr_Hμ = (d2μ_du2 - fd_∇μ_du) / d2μ_du2\n",
    "println(\"Finite difference checn on Hμ again: $(relerr_Hμ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fininte difference check for δμ: 4.738099371069381e-9\n",
      "Fininte difference check for δ∇μ: 9.402574613409843e-9\n"
     ]
    }
   ],
   "source": [
    "# TODO: Finite Difference Check Perturbations of Hyperparameters\n",
    "# δk = dk/dl * l̇\n",
    "function δμ(x, X, c, ẏ, l̇; kernel=k)\n",
    "    δμx = 0.0\n",
    "    KXX = kernel_matrix(kernel, X, X)\n",
    "    KXx = kernel_matrix(kernel, X, reshape(x, 2, 1))\n",
    "    d = KXX \\ KXx\n",
    "    \n",
    "    for i = 1:size(X)[2]\n",
    "        δμx += c[i] * δk(x, X[:, i]) * l̇ + d[i]*ẏ[i]\n",
    "        for j = 1:size(X)[2]\n",
    "            δμx -= d[i] * δk(X[:, i], X[:, j]) * l̇ * c[j] \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return δμx\n",
    "end\n",
    "\n",
    "function δ∇μ(x, X, c, ẏ, l̇; kernel=k)\n",
    "    δ∇μx = zeros(length(x))\n",
    "    W = zeros(size(X))\n",
    "    \n",
    "    for ndx = 1:size(X)[2]\n",
    "        W[:, ndx] = ∇k(x, X[:, ndx])\n",
    "        δ∇μx += δ∇k(x, X[:, ndx]) * c[ndx] * l̇\n",
    "    end\n",
    "    \n",
    "    W /= kernel_matrix(kernel, X, X)\n",
    "    z = copy(ẏ)\n",
    "    \n",
    "    for i = 1:size(X)[2]\n",
    "        for j = 1:size(X)[2]\n",
    "            z[i] -= δk(X[:, i], X[:, j]) * l̇ * c[j]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    δ∇μx += W*z\n",
    "    \n",
    "    return δ∇μx\n",
    "end\n",
    "\n",
    "l̇ = rand()\n",
    "ẏ = rand(length(c))\n",
    "c = kernel_matrix(k, X, X) \\ y\n",
    "\n",
    "gkplush = GaussianKernel(ℓ=1.01+h*l̇, ν²=1.0, σn=0.)\n",
    "gkminush = GaussianKernel(ℓ=1.01-h*l̇, ν²=1.0, σn=0.)\n",
    "kp(x, y) = gkplush(norm(x-y))\n",
    "km(x, y) = gkminush(norm(x-y))\n",
    "\n",
    "cplus = kernel_matrix(kp, X, X) \\ (y + h*ẏ)\n",
    "cminus = kernel_matrix(km, X, X) \\ (y - h*ẏ)\n",
    "\n",
    "δμ_test = δμ(x, X, c, ẏ, l̇)\n",
    "fd_δμ = ( μ(kp, x, X, cplus) - μ(km, x, X, cminus) ) / (2h)\n",
    "relerr = (δμ_test - fd_δμ) / δμ_test\n",
    "println(\"Fininte difference check for δμ: $relerr\")\n",
    "\n",
    "# # Finite difference on δμ\n",
    "u = rand(length(x))\n",
    "δ∇μ_test = u'*δ∇μ(x, X, c, ẏ, l̇)\n",
    "fd_δ∇μ = ( δμ(x+h*u, X, c, ẏ, l̇) - δμ(x-h*u, X, c, ẏ, l̇) ) / (2h)\n",
    "relerr = (δ∇μ_test - fd_δ∇μ) / δ∇μ_test \n",
    "println(\"Fininte difference check for δ∇μ: $relerr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard deviation derivatives\n",
    "\n",
    "The predictive variance is\n",
    "$$\n",
    "  \\sigma^2 = k_{xx} - k_{xX} K_{XX}^{-1} k_{Xx}.\n",
    "$$\n",
    "Differentiating the predictive variance twice in space --- assuming $k_{xx}$ is independent of $x$ by stationarity ---\n",
    "gives us\n",
    "$$\\begin{aligned}\n",
    "  2 \\sigma \\sigma_{,i} &= -2 k_{xX,i} K_{XX}^{-1} k_{Xx} = -2 k_{xX,i} d \\\\\n",
    "  2 \\sigma_{,i} \\sigma_{,j} + 2 \\sigma \\sigma_{,ij} &= -2 k_{xX,ij} K_{XX}^{-1} k_{Xx} - 2 k_{xX,i} K_{XX}^{-1} k_{Xx,j} \\\\\n",
    "                     &= -2 k_{xX,ij} d -2 k_{xX,i} w^{(j)}\n",
    "\\end{aligned}$$\n",
    "Rearranging to get spatial derivatives of $\\sigma$ on their own gives us\n",
    "$$\\begin{aligned}\n",
    "  \\sigma_{,i} &= -\\sigma^{-1} k_{xX,i} d \\\\\n",
    "  \\sigma_{,ij} &= -\\sigma^{-1} \\left[ k_{xX,ij} d + k_{xX,i} w^{(j)} + \\sigma_{,i} \\sigma_{,j} \\right].\n",
    "\\end{aligned}$$\n",
    "\n",
    "Differentiating with respect to data (and locations) and kernel hypers requires more work.  First, note that\n",
    "$$\\begin{aligned}\n",
    "  2 \\sigma \\dot{\\sigma} \n",
    "  &= \\dot{k}_{xx} - 2 \\dot{k}_{xX} K_{XX}^{-1} k_{Xx} + k_{xX} K_{XX}^{-1} \\dot{K}_{XX} K_{XX}^{-1} k_{Xx} \\\\\n",
    "  &= \\dot{k}_{xx} - 2 \\dot{k}_{xX} d + d^T \\dot{K}_{XX} d\n",
    "\\end{aligned}$$\n",
    "Now, differentiating $\\sigma^{-1}$ with respect to data and hypers gives\n",
    "$$\\begin{aligned}\n",
    "  \\dot{\\sigma}_{,i} \n",
    "  &= \\sigma^{-2} \\dot{\\sigma} k_{xX,i} K_{XX}^{-1} k_{Xx} -\n",
    "     \\sigma^{-1} \\left[ \n",
    "       \\dot{k}_{xX,i} K_{XX}^{-1} k_{Xx} +\n",
    "       k_{xX,i} K_{XX}^{-1} \\dot{k}_{Xx} -\n",
    "       k_{xX} K_{XX}^{-1} \\dot{K}_{XX} K_{XX}^{-1} k_{Xx} \\right] \\\\\n",
    "  &= -\\sigma^{-1} \\left[ \\dot{\\sigma} \\sigma_{,i} + \\dot{k}_{xX,i} d + (w^{(i)})^T \\dot{k}_{Xx} - d^T \\dot{K}_{XX} d \\right]\n",
    "\\end{aligned}$$\n",
    "\n",
    "My results when differentiating $\\sigma^{-1}$ with respect to data and hypers gives\n",
    "$$\\begin{aligned}\n",
    "  \\dot{\\sigma}_{,i} &= -\\sigma^{-1} \\left[ \\dot{\\sigma} \\sigma_{,i} -\n",
    "                       (w^{(i)})^T \\dot{K}_{XX} d + (w^{(i)})^T \\dot{k}_{Xx} + \\dot{K}_{xX,i}d\n",
    "  \\right]\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
